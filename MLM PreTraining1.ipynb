{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ae59d2",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Áî±‰∫éÊòØÊñ∞ÁöÑÊñáÁ´†ÊâÄ‰ª•ÈúÄË¶ÅÂÖàËÆ≠ÁªÉ‰∏Ä‰∏™tokenizer\n",
    "\n",
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b2fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./contents/oscar.eo.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "#preprocessing for DatasetforNSP, this is not the original dataset I would be training on. I am running this model as POC which I can adapt to any dataset later\n",
    "#nothing useful or new here just a preprocessing step to make it same as same format I would be using\n",
    "lines3=[] \n",
    "space = 0\n",
    "tot =0\n",
    "for i in lines:\n",
    "    tot = tot+1\n",
    "    if i.strip(\" \")==\"\\n\":\n",
    "        continue\n",
    "    if tot==4:\n",
    "        lines3.append(\"\\n\")\n",
    "        tot=0\n",
    "        continue\n",
    "    lines3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fc455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"oscar.eo.txt\", \"w\", encoding=\"utf-8\")as f:\n",
    "    for i in lines3:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa99b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [\"oscar.eo.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e17648",
   "metadata": {},
   "source": [
    "### Training a tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2675a4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=False,\n",
    ")\n",
    "\n",
    "# And then train\n",
    "tokenizer.train(\n",
    "    paths,\n",
    "    vocab_size=50000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet=1000,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2384a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: Bert: File exists\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bert/voc-vocab.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir Bert\n",
    "tokenizer.save_model(\"Bert\", \"voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56500acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Mi', 'estas', 'Julie', '##n', '.', '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file= \"./contents/Bert/voc-vocab.txt\",\n",
    "    do_lower_case = False,\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "idx = tokenizer.encode(\"Mi estas Julien.\")\n",
    "tokenizer.convert_ids_to_tokens(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e370231",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80192452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ab1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=50000,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12, \n",
    "    num_attention_heads=12, \n",
    "    intermediate_size=3072, \n",
    "    hidden_act='gelu', \n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1, \n",
    "    max_position_embeddings=512, \n",
    "    type_vocab_size=1, \n",
    "    initializer_range=0.02, \n",
    "    layer_norm_eps=1e-12, \n",
    "    pad_token_id=0, \n",
    "    gradient_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14acde45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124492880"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.mlm import BaseMLM\n",
    "\n",
    "model = BaseMLM(vocab_size=50000, pretrain=None)()\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ebac0",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64bbed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maochenhui/miniforge3/envs/DL/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Creating features from dataset file at ./oscar.eo.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of lines:  938226\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "dataset = LineByLineTextDataset(tokenizer=tokenizer, \n",
    "                                file_path=\"./oscar.eo.txt\", \n",
    "                                block_size=512 # maximum sequence length\n",
    "                               )\n",
    "print('No. of lines: ', len(dataset)) # No of lines in your datset\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f75c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 938226\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1466000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1466000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [      5/1466000 02:20 < 19100:50:48, 0.02 it/s, Epoch 0.00/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from train.mlm import MLMTrainer\n",
    "\n",
    "trainer = MLMTrainer(model=model, \n",
    "                     data_collator=data_collator, \n",
    "                     train_dataset=dataset,\n",
    "                     eval_dataset=None\n",
    "                    )\n",
    "t = trainer()\n",
    "t.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989790b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
